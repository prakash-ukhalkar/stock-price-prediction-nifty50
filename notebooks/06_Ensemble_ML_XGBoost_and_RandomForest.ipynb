{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stock Price Prediction - NIFTY 50**\n",
    "\n",
    "### **Notebook 06: Ensemble Machine Learning (XGBoost and Random Forest)**\n",
    "\n",
    "[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/) [![XGBoost](https://img.shields.io/badge/XGBoost-Latest-red)](https://xgboost.readthedocs.io/) [![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-Latest-orange)](https://scikit-learn.org/) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "---\n",
    "\n",
    "**Part of the comprehensive learning series:** [Stock Price Prediction - NIFTY 50](https://github.com/prakash-ukhalkar/stock-price-prediction-nifty50)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Implement ensemble ML algorithms (XGBoost and Random Forest) for financial prediction\n",
    "- Apply tree-based models that handle non-linear relationships without feature scaling\n",
    "- Utilize advanced hyperparameter tuning for gradient boosting optimization\n",
    "- Analyze feature importance from technical indicators and lag variables\n",
    "- Establish strong ensemble baselines before deep learning implementation\n",
    "\n",
    "**Dataset Scope:** Apply ensemble learning to feature-engineered data. Leverage tree-based models for complex pattern recognition.\n",
    "\n",
    "---\n",
    "\n",
    "* This notebook implements **Ensemble Machine Learning** algorithms, focusing on **Random Forest** and **XGBoost**. \n",
    "\n",
    "* These tree-based models excel at capturing complex, non-linear relationships in noisy financial data without requiring feature scaling, establishing a strong high-level benchmark for the comparative analysis. \n",
    "\n",
    "* We utilize the feature-engineered data from Notebook 03."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "We load the feature-engineered training data (`nifty50_train_features.csv`) and the original raw test data (`nifty50_test.csv`), which will be re-engineered here for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features loaded. Shape: (57311, 31)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ta # Needed for test data feature re-engineering\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Define Paths\n",
    "TRAIN_FEATURES_PATH = '../data/processed/nifty50_train_features.csv'\n",
    "TEST_DATA_PATH = '../data/processed/nifty50_test.csv'\n",
    "MODEL_RESULTS_PATH = '../models/ml_model_results.csv'\n",
    "CLASSICAL_RESULTS_PATH = '../models/classical_model_results.csv'\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv(TRAIN_FEATURES_PATH, index_col='Date', parse_dates=True)\n",
    "df_test = pd.read_csv(TEST_DATA_PATH, index_col='Date', parse_dates=True)\n",
    "\n",
    "TARGET_COL = 'Log_Return'\n",
    "\n",
    "# Clean and Define X_train, y_train\n",
    "y_train = df_train[TARGET_COL]\n",
    "X_train = df_train.drop(columns=[TARGET_COL])\n",
    "non_numeric_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "if not non_numeric_cols.empty:\n",
    "    X_train = X_train.drop(columns=non_numeric_cols)\n",
    "\n",
    "print(f\"Training Features loaded. Shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Cross-Validation Setup\n",
    "\n",
    "We use **TimeSeriesSplit** to ensure robust and non-leaky validation during hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized TimeSeriesSplit with 5 folds for tuning.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Initialize TimeSeriesSplit\n",
    "N_SPLITS = 5 \n",
    "tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "\n",
    "print(f\"Initialized TimeSeriesSplit with {N_SPLITS} folds for tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model I: Random Forest Regressor\n",
    "\n",
    "**Explanation:** Random Forest builds an ensemble of decorrelated decision trees, using the average of their predictions. This method is highly effective for reducing the variance and overfitting often seen in single tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Forest GridSearchCV...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Random Forest Best Parameters: {'max_depth': 10, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "Random Forest Best Score (Neg MSE): -2.778991730885983e-07\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Random Forest Hyperparameter Tuning\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Note: Scaling is not required for tree-based models\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100], \n",
    "    'max_depth': [5, 10], \n",
    "    'min_samples_leaf': [5, 10]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=tscv, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting Random Forest GridSearchCV...\")\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "rf_best_model = rf_grid.best_estimator_\n",
    "print(\"\\nRandom Forest Best Parameters:\", rf_grid.best_params_)\n",
    "print(\"Random Forest Best Score (Neg MSE):\", rf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model II: XGBoost Regressor\n",
    "\n",
    "**Explanation:** XGBoost (Extreme Gradient Boosting) is a highly efficient implementation of gradient boosting. It sequentially builds new decision trees to correct the errors of the preceding model, often yielding state-of-the-art results for tabular regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBoost GridSearchCV...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "XGBoost Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "XGBoost Best Score (Neg MSE): -3.841978118487281e-06\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: XGBoost Hyperparameter Tuning\n",
    "\n",
    "xgb_model = XGBRegressor(random_state=42, objective='reg:squarederror', n_jobs=-1)\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100], \n",
    "    'max_depth': [3, 5], \n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=xgb_param_grid,\n",
    "    cv=tscv, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting XGBoost GridSearchCV...\")\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "xgb_best_model = xgb_grid.best_estimator_\n",
    "print(\"\\nXGBoost Best Parameters:\", xgb_grid.best_params_)\n",
    "print(\"XGBoost Best Score (Neg MSE):\", xgb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation on Final Test Set\n",
    "\n",
    "We evaluate the optimized Ensemble models on the unseen test data. As seen in Notebook 05, the raw test data must be re-engineered before prediction to align feature columns with the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data re-engineered and cleaned. Final X_test shape: (14254, 31)\n",
      "Predictions generated for Random Forest and XGBoost on the test set.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Prepare Test Data, Re-Engineer Features, and Generate Predictions\n",
    "\n",
    "# 1. Feature Re-Engineering (MUST match Notebook 03/05 logic)\n",
    "df_test_features = df_test.copy()\n",
    "\n",
    "LAG_PERIODS = [1, 2, 3, 5, 10]\n",
    "for lag in LAG_PERIODS:\n",
    "    df_test_features[f'Close_Lag_{lag}'] = df_test_features['Close'].shift(lag)\n",
    "    df_test_features[f'Return_Lag_{lag}'] = df_test_features['Log_Return'].shift(lag)\n",
    "\n",
    "WINDOW_TREND = [10, 20, 50] \n",
    "for window in WINDOW_TREND:\n",
    "    df_test_features[f'SMA_{window}'] = ta.trend.sma_indicator(df_test_features['Close'], window=window, fillna=False)\n",
    "    df_test_features[f'EMA_{window}'] = ta.trend.ema_indicator(df_test_features['Close'], window=window, fillna=False)\n",
    "macd = ta.trend.MACD(df_test_features['Close'], window_fast=12, window_slow=26, window_sign=9, fillna=False)\n",
    "df_test_features['MACD_Line'] = macd.macd()\n",
    "df_test_features['MACD_Signal'] = macd.macd_signal()\n",
    "RSI_WINDOW = 14 \n",
    "df_test_features[f'RSI_{RSI_WINDOW}'] = ta.momentum.rsi(df_test_features['Close'], window=RSI_WINDOW, fillna=False)\n",
    "df_test_features['MFI'] = ta.volume.money_flow_index(df_test_features['High'], df_test_features['Low'], df_test_features['Close'], df_test_features['Volume'], window=14, fillna=False)\n",
    "df_test_features['ATR'] = ta.volatility.average_true_range(df_test_features['High'], df_test_features['Low'], df_test_features['Close'], window=14, fillna=False)\n",
    "\n",
    "df_test_features = df_test_features.dropna()\n",
    "\n",
    "# 2. Final Data Cleanup and Alignment\n",
    "\n",
    "y_test = df_test_features[TARGET_COL]\n",
    "X_test = df_test_features.drop(columns=[TARGET_COL])\n",
    "\n",
    "# --- CRITICAL FIX: Ensure X_test is cleaned of non-numeric columns and aligned (as trained) ---\n",
    "non_numeric_cols_test = X_test.select_dtypes(include=['object', 'category']).columns\n",
    "if not non_numeric_cols_test.empty:\n",
    "    X_test = X_test.drop(columns=non_numeric_cols_test)\n",
    "\n",
    "# Align the order and presence of features to match the exact set used to train the models (X_train.columns)\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "print(f\"Test data re-engineered and cleaned. Final X_test shape: {X_test.shape}\")\n",
    "\n",
    "# 3. Prediction\n",
    "rf_preds = rf_best_model.predict(X_test)\n",
    "xgb_preds = xgb_best_model.predict(X_test)\n",
    "\n",
    "print(\"Predictions generated for Random Forest and XGBoost on the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ensemble ML Model Evaluation (Log Returns) on Test Set ---\n",
      "                   MSE       MAE      RMSE\n",
      "Model                                     \n",
      "ARIMA         0.000261  0.010981  0.016141\n",
      "SARIMA        0.000260  0.010954  0.016126\n",
      "Prophet       0.000273  0.011414  0.016520\n",
      "KNN           0.000185  0.008851  0.013611\n",
      "SVR           0.000026  0.004653  0.005069\n",
      "RandomForest  0.000004  0.000026  0.001980\n",
      "XGBoost       0.000007  0.000278  0.002640\n",
      "Results consolidated in:  ../models/ml_model_results.csv\n",
      "Proceed to Notebook 07 for Deep Learning models (ANN and LSTM Basics).\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Final Evaluation and Consolidation\n",
    "\n",
    "def evaluate_and_consolidate(y_true, y_pred, model_name, path):\n",
    "    \n",
    "    # Note: Synchronization is simplified since y_true and y_pred are already aligned via the dropna() on the test features.\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    new_results = pd.DataFrame([{'Model': model_name, 'MSE': mse, 'MAE': mae, 'RMSE': rmse}])\n",
    "        \n",
    "    # Load previous results (from classical/traditional ML models) and append new results\n",
    "    if os.path.exists(path):\n",
    "        results_df = pd.read_csv(path)\n",
    "    else:\n",
    "        # Check for classical results file if the ML file is new\n",
    "        if os.path.exists(CLASSICAL_RESULTS_PATH):\n",
    "            results_df = pd.read_csv(CLASSICAL_RESULTS_PATH)\n",
    "        else:\n",
    "            results_df = pd.DataFrame(columns=['Model', 'MSE', 'MAE', 'RMSE'])\n",
    "        \n",
    "    results_df = pd.concat([results_df, new_results], ignore_index=True)\n",
    "    results_df = results_df.drop_duplicates(subset=['Model'], keep='last')\n",
    "    results_df.to_csv(path, index=False)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Evaluate and Save Results\n",
    "evaluate_and_consolidate(y_test, rf_preds, 'RandomForest', MODEL_RESULTS_PATH)\n",
    "final_results = evaluate_and_consolidate(y_test, xgb_preds, 'XGBoost', MODEL_RESULTS_PATH)\n",
    "\n",
    "print(\"--- Ensemble ML Model Evaluation (Log Returns) on Test Set ---\")\n",
    "print(final_results.set_index('Model'))\n",
    "\n",
    "print(\"Results consolidated in: \", MODEL_RESULTS_PATH)\n",
    "print(\"Proceed to Notebook 07 for Deep Learning models (ANN and LSTM Basics).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b63fa59",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "  1. **Ensemble Model Implementation**: Applied Random Forest and XGBoost for advanced prediction\n",
    "\n",
    "  2. **Tree-Based Advantage**: Leveraged models that handle non-linear relationships without scaling\n",
    "\n",
    "  3. **Hyperparameter Optimization**: Used GridSearchCV with TimeSeriesSplit for robust tuning\n",
    "\n",
    "  4. **Feature Engineering Pipeline**: Re-engineered test data to match training feature set\n",
    "\n",
    "  5. **Performance Evaluation**: Established strong ensemble benchmarks for deep learning comparison\n",
    "\n",
    "  6. **Model Consolidation**: Integrated results with previous classical and traditional ML models\n",
    "\n",
    "### Key Ensemble Learning Insights:\n",
    "\n",
    "  - **Random Forest Performance**: Bootstrap aggregation reduced overfitting and improved generalization\n",
    "  \n",
    "  - **XGBoost Effectiveness**: Gradient boosting captured complex feature interactions efficiently\n",
    "  \n",
    "  - **Feature Handling**: Tree-based models naturally handled mixed-scale technical indicators\n",
    "  \n",
    "  - **Non-Linear Patterns**: Ensemble methods effectively modeled complex financial relationships\n",
    "  \n",
    "  - **Computational Efficiency**: Parallel processing enabled faster hyperparameter optimization\n",
    "\n",
    "### Technical Implementation Notes:\n",
    "\n",
    "  - **No Feature Scaling**: Tree-based models eliminated preprocessing requirements\n",
    "  \n",
    "  - **Advanced Hyperparameters**: Tuned depth, estimators, and regularization parameters\n",
    "  \n",
    "  - **Feature Engineering**: Consistent technical indicator calculation across train/test sets\n",
    "  \n",
    "  - **Cross-Validation**: Maintained temporal integrity during model selection\n",
    "\n",
    "### Model Performance Framework:\n",
    "\n",
    "  - **Benchmark Establishment**: Created strong ensemble baselines for deep learning comparison\n",
    "  \n",
    "  - **Feature Importance**: Tree-based models provide interpretable feature rankings\n",
    "  \n",
    "  - **Robustness**: Ensemble methods showed resilience to financial data noise\n",
    "  \n",
    "  - **Scalability**: Models demonstrated computational efficiency on large datasets\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**Notebook 07**: We'll transition to Deep Learning models including:\n",
    "- Artificial Neural Networks (ANN) for non-linear pattern recognition\n",
    "- Basic LSTM implementation for sequential modeling\n",
    "- Advanced neural architectures for time series prediction\n",
    "- Comparison of deep learning with ensemble benchmarks\n",
    "\n",
    "---\n",
    "\n",
    "### *Next Notebook Preview*\n",
    "\n",
    "Having established strong ensemble learning benchmarks, we'll now explore deep learning's potential to capture even more complex temporal patterns and non-linear relationships in our financial time series data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd0089",
   "metadata": {},
   "source": [
    "#### About This Project\n",
    "\n",
    "This notebook is part of the **Stock Price Prediction - NIFTY 50** repository - a comprehensive machine learning pipeline for predicting stock prices using classical to advanced techniques including ARIMA, LSTM, XGBoost, and evolutionary optimization.\n",
    "\n",
    "**Repository:** [`stock-price-prediction-nifty50`](https://github.com/prakash-ukhalkar/stock-price-prediction-nifty50)\n",
    "\n",
    "**Project Features:**\n",
    "- **12 Sequential Notebooks**: From data acquisition to deployment\n",
    "- **Multiple Model Types**: Classical (ARIMA), Traditional ML (SVR, XGBoost), Deep Learning (LSTM, BiLSTM)  \n",
    "- **Advanced Optimization**: Genetic Algorithm and Simulated Annealing\n",
    "- **Production Ready**: Streamlit dashboard and trading strategy backtesting\n",
    "\n",
    "**Notebook Sequence:**\n",
    "1. COMPLETE - Data Acquisition and Preprocessing\n",
    "2. COMPLETE - EDA and Time Series Foundations\n",
    "3. COMPLETE - Feature Engineering and Technical Analysis\n",
    "4. COMPLETE - Classical Models (ARIMA, SARIMA, Prophet)\n",
    "5. COMPLETE - Traditional ML (KNN and SVR)\n",
    "6. COMPLETE - **Ensemble ML (XGBoost, Random Forest)** (Current)\n",
    "7. NEXT - Deep Learning Basics (ANN, LSTM)\n",
    "8. PENDING - Advanced Deep Learning (BiLSTM, GRU)\n",
    "\n",
    "#### **Author**\n",
    "\n",
    "**Prakash Ukhalkar**  \n",
    "[![GitHub](https://img.shields.io/badge/GitHub-prakash--ukhalkar-blue?style=flat&logo=github)](https://github.com/prakash-ukhalkar)\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "  <sub>Built with care for the quantitative finance and data science community</sub>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_N50",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
