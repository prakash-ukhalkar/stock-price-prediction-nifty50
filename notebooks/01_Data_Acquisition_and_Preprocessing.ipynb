{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6b3b84",
   "metadata": {},
   "source": [
    "# Stock Price Prediction - NIFTY 50\n",
    "## Notebook 01: Data Acquisition and Preprocessing\n",
    "\n",
    "**Objective:** Fetch, merge, clean NIFTY50 data (2020-2025). Calculate Log Returns.\n",
    "\n",
    "### Learning Outcomes:\n",
    "- Master financial data acquisition from NSE using `yfinance` and `nsepy`\n",
    "- Understand data preprocessing techniques for financial time series\n",
    "- Learn to calculate and interpret log returns\n",
    "- Implement robust data cleaning and validation procedures\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47cfc5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll start by importing all necessary libraries for data acquisition and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aebde6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Acquisition\n",
    "import yfinance as yf\n",
    "from nsepy import get_history\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# System and File Operations\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "print(\"üìä Libraries imported successfully!\")\n",
    "print(f\"üìÖ Current date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19eeb0c",
   "metadata": {},
   "source": [
    "## 2. Define NIFTY 50 Stock Symbols\n",
    "\n",
    "Let's define the current NIFTY 50 constituents for data acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIFTY 50 Stock Symbols (Updated as of 2024)\n",
    "NIFTY_50_SYMBOLS = [\n",
    "    'RELIANCE.NS', 'TCS.NS', 'HDFCBANK.NS', 'ICICIBANK.NS', 'HINDUNILVR.NS',\n",
    "    'INFY.NS', 'ITC.NS', 'SBIN.NS', 'BHARTIARTL.NS', 'ASIANPAINT.NS',\n",
    "    'MARUTI.NS', 'HCLTECH.NS', 'AXISBANK.NS', 'LT.NS', 'SUNPHARMA.NS',\n",
    "    'TITAN.NS', 'ULTRACEMCO.NS', 'WIPRO.NS', 'NESTLEIND.NS', 'POWERGRID.NS',\n",
    "    'NTPC.NS', 'TECHM.NS', 'ONGC.NS', 'M&M.NS', 'TATAMOTORS.NS',\n",
    "    'KOTAKBANK.NS', 'HDFCLIFE.NS', 'BAJFINANCE.NS', 'SBILIFE.NS', 'DRREDDY.NS',\n",
    "    'INDUSINDBK.NS', 'ADANIENT.NS', 'GRASIM.NS', 'CIPLA.NS', 'BRITANNIA.NS',\n",
    "    'COALINDIA.NS', 'TATASTEEL.NS', 'APOLLOHOSP.NS', 'HINDALCO.NS', 'DIVISLAB.NS',\n",
    "    'HEROMOTOCO.NS', 'ADANIPORTS.NS', 'UPL.NS', 'BAJAJFINSV.NS', 'JSWSTEEL.NS',\n",
    "    'EICHERMOT.NS', 'TATACONSUM.NS', 'LTIM.NS', 'BAJAJ-AUTO.NS', 'BPCL.NS'\n",
    "]\n",
    "\n",
    "print(f\"üéØ Total NIFTY 50 symbols defined: {len(NIFTY_50_SYMBOLS)}\")\n",
    "print(\"\\nüìã Sample symbols:\")\n",
    "for i, symbol in enumerate(NIFTY_50_SYMBOLS[:10]):\n",
    "    print(f\"{i+1:2d}. {symbol}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa3e8ed",
   "metadata": {},
   "source": [
    "## 3. Data Acquisition Functions\n",
    "\n",
    "Let's create robust functions to fetch stock data with error handling and retry mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb9f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(symbol, start_date='2020-01-01', end_date=None, max_retries=3):\n",
    "    \"\"\"\n",
    "    Fetch stock data for a given symbol using yfinance.\n",
    "    \n",
    "    Parameters:\n",
    "    - symbol: Stock symbol (e.g., 'RELIANCE.NS')\n",
    "    - start_date: Start date for data (default: '2020-01-01')\n",
    "    - end_date: End date for data (default: today)\n",
    "    - max_retries: Maximum number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with stock data or None if failed\n",
    "    \"\"\"\n",
    "    if end_date is None:\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Fetch data using yfinance\n",
    "            ticker = yf.Ticker(symbol)\n",
    "            data = ticker.history(start=start_date, end=end_date)\n",
    "            \n",
    "            if not data.empty:\n",
    "                # Add symbol column\n",
    "                data['Symbol'] = symbol.replace('.NS', '')\n",
    "                data.reset_index(inplace=True)\n",
    "                \n",
    "                print(f\"‚úÖ Successfully fetched {len(data)} records for {symbol}\")\n",
    "                return data\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  No data found for {symbol}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Attempt {attempt + 1} failed for {symbol}: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"üîÑ Retrying in 2 seconds...\")\n",
    "                import time\n",
    "                time.sleep(2)\n",
    "    \n",
    "    print(f\"‚ùå Failed to fetch data for {symbol} after {max_retries} attempts\")\n",
    "    return None\n",
    "\n",
    "def fetch_nifty_index_data(start_date='2020-01-01', end_date=None):\n",
    "    \"\"\"\n",
    "    Fetch NIFTY 50 index data.\n",
    "    \"\"\"\n",
    "    print(\"üìà Fetching NIFTY 50 Index data...\")\n",
    "    return fetch_stock_data('^NSEI', start_date, end_date)\n",
    "\n",
    "print(\"üîß Data acquisition functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5203e8",
   "metadata": {},
   "source": [
    "## 4. Batch Data Collection\n",
    "\n",
    "Now let's fetch data for all NIFTY 50 stocks with progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set date range for data collection\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"üìä Starting data collection for NIFTY 50 stocks\")\n",
    "print(f\"üìÖ Date range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"üéØ Total symbols to process: {len(NIFTY_50_SYMBOLS)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Initialize containers\n",
    "all_stock_data = []\n",
    "failed_symbols = []\n",
    "successful_symbols = []\n",
    "\n",
    "# Fetch data for each symbol\n",
    "for i, symbol in enumerate(NIFTY_50_SYMBOLS, 1):\n",
    "    print(f\"\\nüìà [{i:2d}/{len(NIFTY_50_SYMBOLS)}] Processing {symbol}...\")\n",
    "    \n",
    "    data = fetch_stock_data(symbol, START_DATE, END_DATE)\n",
    "    \n",
    "    if data is not None and not data.empty:\n",
    "        all_stock_data.append(data)\n",
    "        successful_symbols.append(symbol)\n",
    "    else:\n",
    "        failed_symbols.append(symbol)\n",
    "    \n",
    "    # Progress update every 10 stocks\n",
    "    if i % 10 == 0:\n",
    "        print(f\"\\nüèÅ Progress: {i}/{len(NIFTY_50_SYMBOLS)} completed ({i/len(NIFTY_50_SYMBOLS)*100:.1f}%)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä DATA COLLECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Successful: {len(successful_symbols)} stocks\")\n",
    "print(f\"‚ùå Failed: {len(failed_symbols)} stocks\")\n",
    "print(f\"üìà Success Rate: {len(successful_symbols)/len(NIFTY_50_SYMBOLS)*100:.1f}%\")\n",
    "\n",
    "if failed_symbols:\n",
    "    print(f\"\\n‚ùå Failed symbols: {', '.join(failed_symbols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb7af82",
   "metadata": {},
   "source": [
    "## 5. Combine and Structure Data\n",
    "\n",
    "Let's combine all individual stock data into a master DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all stock data\n",
    "if all_stock_data:\n",
    "    # Concatenate all DataFrames\n",
    "    combined_data = pd.concat(all_stock_data, ignore_index=True)\n",
    "    \n",
    "    # Sort by Symbol and Date\n",
    "    combined_data = combined_data.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "    \n",
    "    print(\"üìä MASTER DATASET CREATED\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"üìà Total Records: {len(combined_data):,}\")\n",
    "    print(f\"üè¢ Unique Stocks: {combined_data['Symbol'].nunique()}\")\n",
    "    print(f\"üìÖ Date Range: {combined_data['Date'].min()} to {combined_data['Date'].max()}\")\n",
    "    print(f\"üíæ Memory Usage: {combined_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(\"\\nüìã Sample Data:\")\n",
    "    display(combined_data.head(10))\n",
    "    \n",
    "    # Data info\n",
    "    print(\"\\nüìä Dataset Info:\")\n",
    "    combined_data.info()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data collected. Please check your internet connection and symbol list.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca7436",
   "metadata": {},
   "source": [
    "## 6. Data Quality Assessment\n",
    "\n",
    "Let's assess the quality of our collected data and identify any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c936f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Assessment\n",
    "print(\"üîç DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Missing values analysis\n",
    "missing_data = combined_data.isnull().sum()\n",
    "print(\"\\n‚ùì Missing Values:\")\n",
    "for col, count in missing_data.items():\n",
    "    if count > 0:\n",
    "        percentage = (count / len(combined_data)) * 100\n",
    "        print(f\"  {col}: {count:,} ({percentage:.2f}%)\")\n",
    "\n",
    "# Check for duplicate records\n",
    "duplicates = combined_data.duplicated(['Symbol', 'Date']).sum()\n",
    "print(f\"\\nüîÑ Duplicate Records: {duplicates}\")\n",
    "\n",
    "# Data completeness by symbol\n",
    "data_completeness = combined_data.groupby('Symbol').size().describe()\n",
    "print(\"\\nüìä Data Completeness by Symbol:\")\n",
    "print(data_completeness)\n",
    "\n",
    "# Check for negative or zero prices (data anomalies)\n",
    "price_anomalies = {\n",
    "    'Negative Open': (combined_data['Open'] <= 0).sum(),\n",
    "    'Negative High': (combined_data['High'] <= 0).sum(),\n",
    "    'Negative Low': (combined_data['Low'] <= 0).sum(),\n",
    "    'Negative Close': (combined_data['Close'] <= 0).sum(),\n",
    "    'Zero Volume': (combined_data['Volume'] == 0).sum()\n",
    "}\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Price Anomalies:\")\n",
    "for anomaly, count in price_anomalies.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {anomaly}: {count}\")\n",
    "    else:\n",
    "        print(f\"  {anomaly}: ‚úÖ None found\")\n",
    "\n",
    "# Date range consistency\n",
    "date_ranges = combined_data.groupby('Symbol')['Date'].agg(['min', 'max', 'count'])\n",
    "print(\"\\nüìÖ Date Range Summary:\")\n",
    "print(f\"  Earliest Date: {date_ranges['min'].min()}\")\n",
    "print(f\"  Latest Date: {date_ranges['max'].max()}\")\n",
    "print(f\"  Min Records/Stock: {date_ranges['count'].min()}\")\n",
    "print(f\"  Max Records/Stock: {date_ranges['count'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab28bc05",
   "metadata": {},
   "source": [
    "## 7. Calculate Log Returns\n",
    "\n",
    "**Fundamental Concept:** Raw stock prices are often **non-stationary**, violating assumptions of many statistical models. By contrast, **Daily Returns** are generally **stationary**, making them a more stable and reliable variable for forecasting. We calculate the **Log Returns** using the natural logarithm.\n",
    "\n",
    "The mathematical formula used for Log Returns is: $LogReturn_t = \\ln(\\frac{Close_t}{Close_{t-1}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_returns(df):\n",
    "    \"\"\"\n",
    "    Calculate log returns for each stock in the dataset.\n",
    "    \n",
    "    Log Return = ln(Price_t / Price_t-1)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['Symbol', 'Date'])\n",
    "    \n",
    "    # Calculate log returns for each stock separately\n",
    "    df['Log_Return'] = df.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: np.log(x / x.shift(1))\n",
    "    )\n",
    "    \n",
    "    # Calculate simple returns as well for comparison\n",
    "    df['Simple_Return'] = df.groupby('Symbol')['Close'].pct_change()\n",
    "    \n",
    "    # Calculate additional return metrics\n",
    "    df['Price_Change'] = df.groupby('Symbol')['Close'].diff()\n",
    "    df['Price_Change_Pct'] = df['Price_Change'] / df.groupby('Symbol')['Close'].shift(1) * 100\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply log return calculation\n",
    "print(\"üìä Calculating Log Returns...\")\n",
    "combined_data_with_returns = calculate_log_returns(combined_data)\n",
    "\n",
    "# Remove first day for each stock (NaN values due to lag calculation)\n",
    "combined_data_with_returns = combined_data_with_returns.dropna(subset=['Log_Return'])\n",
    "\n",
    "print(\"‚úÖ Log Returns calculated successfully!\")\n",
    "print(f\"üìà Records with returns: {len(combined_data_with_returns):,}\")\n",
    "\n",
    "# Display sample with returns\n",
    "print(\"\\nüìã Sample Data with Returns:\")\n",
    "sample_cols = ['Date', 'Symbol', 'Close', 'Log_Return', 'Simple_Return', 'Price_Change_Pct']\n",
    "display(combined_data_with_returns[sample_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c09e0",
   "metadata": {},
   "source": [
    "## 8. Return Statistics and Analysis\n",
    "\n",
    "Let's analyze the statistical properties of our calculated returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate return statistics by symbol\n",
    "return_stats = combined_data_with_returns.groupby('Symbol')['Log_Return'].agg([\n",
    "    'count', 'mean', 'std', 'min', 'max', 'skew', \n",
    "    lambda x: x.quantile(0.25),  # Q1\n",
    "    'median',                     # Q2\n",
    "    lambda x: x.quantile(0.75)   # Q3\n",
    "]).round(6)\n",
    "\n",
    "# Rename lambda columns\n",
    "return_stats.columns = ['Count', 'Mean', 'Std', 'Min', 'Max', 'Skewness', 'Q1', 'Median', 'Q3']\n",
    "\n",
    "# Add additional metrics\n",
    "return_stats['Annualized_Return'] = return_stats['Mean'] * 252  # 252 trading days\n",
    "return_stats['Annualized_Volatility'] = return_stats['Std'] * np.sqrt(252)\n",
    "return_stats['Sharpe_Ratio'] = return_stats['Annualized_Return'] / return_stats['Annualized_Volatility']\n",
    "\n",
    "print(\"üìä LOG RETURN STATISTICS BY STOCK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display top 10 stocks by Sharpe ratio\n",
    "print(\"\\nüèÜ Top 10 Stocks by Sharpe Ratio:\")\n",
    "top_sharpe = return_stats.nlargest(10, 'Sharpe_Ratio')[['Mean', 'Std', 'Annualized_Return', 'Sharpe_Ratio']]\n",
    "display(top_sharpe)\n",
    "\n",
    "# Overall market statistics\n",
    "overall_stats = combined_data_with_returns['Log_Return'].agg([\n",
    "    'count', 'mean', 'std', 'min', 'max', 'skew', 'kurtosis'\n",
    "])\n",
    "\n",
    "print(\"\\nüìà Overall Market Statistics:\")\n",
    "print(f\"  Total Observations: {overall_stats['count']:,}\")\n",
    "print(f\"  Mean Log Return: {overall_stats['mean']:.6f} ({overall_stats['mean']*252:.4f} annualized)\")\n",
    "print(f\"  Volatility (Std): {overall_stats['std']:.6f} ({overall_stats['std']*np.sqrt(252):.4f} annualized)\")\n",
    "print(f\"  Minimum Return: {overall_stats['min']:.6f} ({overall_stats['min']*100:.2f}%)\")\n",
    "print(f\"  Maximum Return: {overall_stats['max']:.6f} ({overall_stats['max']*100:.2f}%)\")\n",
    "print(f\"  Skewness: {overall_stats['skew']:.4f}\")\n",
    "print(f\"  Kurtosis: {overall_stats['kurtosis']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b0e9a",
   "metadata": {},
   "source": [
    "## 9. Data Visualization\n",
    "\n",
    "Let's create comprehensive visualizations to understand our data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf71d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Log Returns Distribution (All Stocks)',\n",
    "        'Sample Stock Price Evolution', \n",
    "        'Returns Volatility by Stock',\n",
    "        'Cumulative Returns (Top 5 by Sharpe)'\n",
    "    ],\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# 1. Log Returns Distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=combined_data_with_returns['Log_Return'],\n",
    "        nbinsx=100,\n",
    "        name='Log Returns',\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Sample Stock Price Evolution (Reliance)\n",
    "reliance_data = combined_data_with_returns[combined_data_with_returns['Symbol'] == 'RELIANCE'].copy()\n",
    "if not reliance_data.empty:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=reliance_data['Date'],\n",
    "            y=reliance_data['Close'],\n",
    "            mode='lines',\n",
    "            name='RELIANCE Close Price',\n",
    "            line=dict(color='blue')\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# 3. Returns Volatility by Stock\n",
    "top_10_vol = return_stats.nlargest(10, 'Std')\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=top_10_vol.index,\n",
    "        y=top_10_vol['Std'],\n",
    "        name='Daily Volatility',\n",
    "        marker_color='red',\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Cumulative Returns for Top 5 Sharpe Ratio stocks\n",
    "top_5_symbols = return_stats.nlargest(5, 'Sharpe_Ratio').index.tolist()\n",
    "colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
    "\n",
    "for i, symbol in enumerate(top_5_symbols):\n",
    "    stock_data = combined_data_with_returns[combined_data_with_returns['Symbol'] == symbol].copy()\n",
    "    if not stock_data.empty:\n",
    "        stock_data = stock_data.sort_values('Date')\n",
    "        stock_data['Cumulative_Return'] = (1 + stock_data['Log_Return']).cumprod() - 1\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=stock_data['Date'],\n",
    "                y=stock_data['Cumulative_Return'] * 100,\n",
    "                mode='lines',\n",
    "                name=f'{symbol}',\n",
    "                line=dict(color=colors[i])\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"NIFTY 50 Data Analysis Dashboard\",\n",
    "    title_x=0.5,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "fig.update_xaxes(title_text=\"Log Return\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Date\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Price (‚Çπ)\", row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Stock Symbol\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Daily Volatility\", row=2, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Date\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Cumulative Return (%)\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"üìä Comprehensive data visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd230fb2",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data\n",
    "\n",
    "Let's save our cleaned and processed data for use in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5713fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "data_dir = Path('../data/processed')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the processed data\n",
    "processed_file = data_dir / 'nifty50_processed_data.csv'\n",
    "combined_data_with_returns.to_csv(processed_file, index=False)\n",
    "\n",
    "# Save return statistics\n",
    "stats_file = data_dir / 'nifty50_return_statistics.csv'\n",
    "return_stats.to_csv(stats_file)\n",
    "\n",
    "# Create a summary file\n",
    "summary_info = {\n",
    "    'total_records': len(combined_data_with_returns),\n",
    "    'unique_symbols': combined_data_with_returns['Symbol'].nunique(),\n",
    "    'date_range_start': str(combined_data_with_returns['Date'].min()),\n",
    "    'date_range_end': str(combined_data_with_returns['Date'].max()),\n",
    "    'successful_symbols': len(successful_symbols),\n",
    "    'failed_symbols': len(failed_symbols),\n",
    "    'processing_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary_info])\n",
    "summary_file = data_dir / 'data_processing_summary.csv'\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "\n",
    "print(\"üíæ DATA SAVED SUCCESSFULLY\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"üìÅ Main Dataset: {processed_file}\")\n",
    "print(f\"üìä Statistics: {stats_file}\")\n",
    "print(f\"üìã Summary: {summary_file}\")\n",
    "print(f\"\\nüìà Records Saved: {len(combined_data_with_returns):,}\")\n",
    "print(f\"üíæ File Size: {processed_file.stat().st_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae78f05",
   "metadata": {},
   "source": [
    "## üìã Summary\n",
    "\n",
    "### ‚úÖ What We Accomplished:\n",
    "\n",
    "1. **Data Acquisition**: Successfully fetched NIFTY 50 stock data from 2020-2025\n",
    "2. **Data Quality**: Assessed and validated data quality with comprehensive checks\n",
    "3. **Log Returns**: Calculated log returns for stationarity and modeling readiness\n",
    "4. **Statistical Analysis**: Computed return statistics, volatility, and Sharpe ratios\n",
    "5. **Visualization**: Created comprehensive dashboards for data exploration\n",
    "6. **Data Export**: Saved processed data for subsequent analysis notebooks\n",
    "\n",
    "### üìä Key Insights:\n",
    "\n",
    "- **Dataset Size**: Processed thousands of records across 50 stocks\n",
    "- **Time Period**: 5+ years of comprehensive market data\n",
    "- **Return Characteristics**: Analyzed risk-return profiles of individual stocks\n",
    "- **Data Quality**: Implemented robust error handling and quality assurance\n",
    "\n",
    "### üîÑ Next Steps:\n",
    "\n",
    "**Notebook 02**: We'll perform Exploratory Data Analysis (EDA) and Time Series Foundations including:\n",
    "- Stationarity testing (ADF tests)\n",
    "- ACF/PACF analysis\n",
    "- Time-based data splitting\n",
    "- Trend and seasonality analysis\n",
    "\n",
    "---\n",
    "\n",
    "**üìù Note**: This notebook establishes the foundation for all subsequent analysis. The log returns calculated here will be our primary target variable for prediction models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
