{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2fae8da",
   "metadata": {},
   "source": [
    "## **Stock Price Prediction - NIFTY 50**\n",
    "\n",
    "### **Notebook 07: Deep Learning I (ANN and LSTM Basics)**\n",
    "\n",
    "[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/) [![TensorFlow](https://img.shields.io/badge/TensorFlow-2.10%2B-orange)](https://tensorflow.org/) [![Keras](https://img.shields.io/badge/Keras-Latest-red)](https://keras.io/) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "---\n",
    "\n",
    "**Part of the comprehensive learning series:** [Stock Price Prediction - NIFTY 50](https://github.com/prakash-ukhalkar/stock-price-prediction-nifty50)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Implement fundamental deep learning architectures (ANN and LSTM) for financial prediction\n",
    "- Apply neural networks to capture dynamic and chaotic market patterns\n",
    "- Compare tabular vs sequential modeling approaches on same dataset\n",
    "- Establish deep learning baselines for advanced architectures\n",
    "- Understand temporal dependency modeling with LSTM networks\n",
    "\n",
    "**Dataset Scope:** Apply deep learning to feature-engineered data and sequential price patterns. Compare ANN and LSTM approaches.\n",
    "\n",
    "---\n",
    "\n",
    "* This notebook begins the Deep Learning phase of the research, which is essential for capturing the **dynamic, inconsistent, and chaotic** nature of the stock market. We implement two fundamental architectures cited in the literature: \n",
    " \n",
    "  1.  A standard **Artificial Neural Network (ANN)**, treating the input features (TAs, Lags) as a static tabular problem.\n",
    "  \n",
    "  2.  A foundational **Univariate Long Short-Term Memory (LSTM)** network, designed to learn temporal dependencies using sequential price data. \n",
    "\n",
    "* Both models predict the next day's **Log Return**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8ccad",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "We load the feature-engineered training data (`nifty50_train_features.csv`) and the raw test data (`nifty50_test.csv`). We use **TensorFlow/Keras** for building and training the neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9748be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data loaded. Shape: (57311, 33)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ta # Re-imported for feature manipulation if needed\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define Paths\n",
    "TRAIN_FEATURES_PATH = '../data/processed/nifty50_train_features.csv'\n",
    "TEST_DATA_PATH = '../data/processed/nifty50_test.csv'\n",
    "MODEL_RESULTS_PATH = '../models/dl_model_results.csv' # New results file for DL models\n",
    "ML_RESULTS_PATH = '../models/ml_model_results.csv' # For consolidation\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv(TRAIN_FEATURES_PATH, index_col='Date', parse_dates=True)\n",
    "df_test = pd.read_csv(TEST_DATA_PATH, index_col='Date', parse_dates=True)\n",
    "\n",
    "TARGET_COL = 'Log_Return'\n",
    "print(f\"Training Data loaded. Shape: {df_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab51cb2",
   "metadata": {},
   "source": [
    "## 2. Model I: Artificial Neural Network (ANN)\n",
    "\n",
    "**Explanation:** The ANN serves as the first Deep Learning benchmark. It treats the feature-engineered data like a standard tabular dataset. This tests whether prediction is driven primarily by the feature set rather than memory. **Scaling is mandatory for ANNs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c99638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping non-numeric columns from X_train_ann: ['Symbol']\n",
      "ANN Input Dimension (Features): 31\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: ANN Data Preparation and Scaling (FIXED)\n",
    "\n",
    "X_train_ann = df_train.drop(columns=[TARGET_COL])\n",
    "y_train_ann = df_train[TARGET_COL].values\n",
    "\n",
    "# --- CRITICAL FIX: Clean non-numeric columns (Resolves 'ValueError: could not convert string to float') ---\n",
    "non_numeric_cols = X_train_ann.select_dtypes(include=['object', 'category']).columns\n",
    "if not non_numeric_cols.empty:\n",
    "    print(f\"Dropping non-numeric columns from X_train_ann: {non_numeric_cols.tolist()}\")\n",
    "    X_train_ann = X_train_ann.drop(columns=non_numeric_cols)\n",
    "    \n",
    "# Scale Features\n",
    "scaler_ann = MinMaxScaler()\n",
    "X_train_ann_scaled = scaler_ann.fit_transform(X_train_ann)\n",
    "X_train_ann_scaled = pd.DataFrame(X_train_ann_scaled, index=X_train_ann.index, columns=X_train_ann.columns)\n",
    "\n",
    "INPUT_DIM_ANN = X_train_ann_scaled.shape[1]\n",
    "\n",
    "print(f\"ANN Input Dimension (Features): {INPUT_DIM_ANN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "141bcecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ANN training...\n",
      "ANN Training complete.\n",
      "ANN Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Build and Train ANN Model\n",
    "\n",
    "ann_model = Sequential([\n",
    "    InputLayer(shape=(INPUT_DIM_ANN,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1) # Output layer for regression\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(\"Starting ANN training...\")\n",
    "ann_history = ann_model.fit(\n",
    "    X_train_ann_scaled, y_train_ann,\n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"ANN Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe0486",
   "metadata": {},
   "source": [
    "### 2.1 ANN Prediction on Test Set\n",
    "We prepare the test data by ensuring feature re-engineering, cleanup, and scaling are applied consistently with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "442aa65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "ANN Predictions generated. Test features used: 14254\n",
      "ANN Predictions generated. Test features used: 14254\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: ANN Prediction Data Prep and Prediction\n",
    "\n",
    "# 1. Feature Re-Engineering (MUST be done on raw test data)\n",
    "df_test_ann_features = df_test.copy()\n",
    "LAG_PERIODS = [1, 2, 3, 5, 10]\n",
    "for lag in LAG_PERIODS:\n",
    "    df_test_ann_features[f'Close_Lag_{lag}'] = df_test_ann_features['Close'].shift(lag)\n",
    "    df_test_ann_features[f'Return_Lag_{lag}'] = df_test_ann_features['Log_Return'].shift(lag)\n",
    "WINDOW_TREND = [10, 20, 50] \n",
    "for window in WINDOW_TREND:\n",
    "    df_test_ann_features[f'SMA_{window}'] = ta.trend.sma_indicator(df_test_ann_features['Close'], window=window, fillna=False)\n",
    "    df_test_ann_features[f'EMA_{window}'] = ta.trend.ema_indicator(df_test_ann_features['Close'], window=window, fillna=False)\n",
    "macd = ta.trend.MACD(df_test_ann_features['Close'], window_fast=12, window_slow=26, window_sign=9, fillna=False)\n",
    "df_test_ann_features['MACD_Line'] = macd.macd()\n",
    "df_test_ann_features['MACD_Signal'] = macd.macd_signal()\n",
    "RSI_WINDOW = 14 \n",
    "df_test_ann_features[f'RSI_{RSI_WINDOW}'] = ta.momentum.rsi(df_test_ann_features['Close'], window=RSI_WINDOW, fillna=False)\n",
    "df_test_ann_features['MFI'] = ta.volume.money_flow_index(df_test_ann_features['High'], df_test_ann_features['Low'], df_test_ann_features['Close'], df_test_ann_features['Volume'], window=14, fillna=False)\n",
    "df_test_ann_features['ATR'] = ta.volatility.average_true_range(df_test_ann_features['High'], df_test_ann_features['Low'], df_test_ann_features['Close'], window=14, fillna=False)\n",
    "df_test_ann_features = df_test_ann_features.dropna()\n",
    "\n",
    "# 2. Final Cleanup and Alignment\n",
    "X_test_ann = df_test_ann_features.drop(columns=[TARGET_COL])\n",
    "\n",
    "# Drop non-numeric columns from X_test to match trained feature set\n",
    "non_numeric_cols_test = X_test_ann.select_dtypes(include=['object', 'category']).columns\n",
    "X_test_ann = X_test_ann.drop(columns=non_numeric_cols_test)\n",
    "\n",
    "# Align columns and save feature list for later evaluation (needed for y_true extraction)\n",
    "X_test_ann = X_test_ann[X_train_ann.columns] \n",
    "\n",
    "# 3. Scale and Predict\n",
    "X_test_ann_scaled = scaler_ann.transform(X_test_ann)\n",
    "ann_preds = ann_model.predict(X_test_ann_scaled).flatten()\n",
    "\n",
    "print(f\"ANN Predictions generated. Test features used: {X_test_ann.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a6fd2",
   "metadata": {},
   "source": [
    "## 3. Model II: Univariate LSTM (Long Short-Term Memory)\n",
    "\n",
    "**Explanation:** LSTM is designed to capture **temporal dynamics**. We use a **univariate** approach here, using only the lagged Close Price to predict the next return, focusing entirely on sequential memory. This requires **data windowing** and **3D input reshaping**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e71563",
   "metadata": {},
   "source": [
    "### 3.1 Data Windowing and Reshaping\n",
    "We create sequences of `TIME_STEPS` (lookback window) of scaled price data to predict the next day's Log Return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cde63c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Train X shape (samples, timesteps, features): (57291, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: LSTM Data Windowing Function and Training Prep\n",
    "\n",
    "TIME_STEPS = 20 # Lookback window: last 20 trading days\n",
    "\n",
    "def create_lstm_dataset(data, time_steps):\n",
    "    X, y_index = [], []\n",
    "    # data should be the scaled feature (Close Price) series/df\n",
    "    for i in range(len(data) - time_steps):\n",
    "        # Features X: The last 'time_steps' of scaled Close Prices\n",
    "        X.append(data.iloc[i:(i + time_steps), 0].values) \n",
    "        # Target y_index: The index (Date) immediately following the sequence\n",
    "        y_index.append(data.iloc[i + time_steps].name)\n",
    "    return np.array(X), np.array(y_index)\n",
    "\n",
    "# Prepare Training Data (Scale Close Price)\n",
    "df_lstm_train = df_train[['Close']].copy()\n",
    "scaler_lstm = MinMaxScaler()\n",
    "df_lstm_train['Close_Scaled'] = scaler_lstm.fit_transform(df_lstm_train[['Close']])\n",
    "\n",
    "X_train_lstm, y_train_index = create_lstm_dataset(df_lstm_train.drop(columns=['Close']), TIME_STEPS)\n",
    "\n",
    "# Map index back to y_train to get the actual target values\n",
    "y_train_lstm = df_train.loc[y_train_index, TARGET_COL].values\n",
    "\n",
    "# Reshape X for LSTM input: [samples, timesteps, features=1]\n",
    "X_train_lstm = np.reshape(X_train_lstm, (X_train_lstm.shape[0], X_train_lstm.shape[1], 1))\n",
    "\n",
    "print(f\"LSTM Train X shape (samples, timesteps, features): {X_train_lstm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b35f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Test X shape: (14340, 20, 1)\n",
      "LSTM Test y shape: (716600,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Prepare Test Data for LSTM\n",
    "\n",
    "# Need a continuous sequence of data encompassing the lookback window plus the test period.\n",
    "df_full = pd.concat([df_train[['Close']], df_test[['Close']]], axis=0)\n",
    "\n",
    "# Scale the Close Price (using scaler_lstm fitted on train data)\n",
    "df_full['Close_Scaled'] = scaler_lstm.transform(df_full[['Close']])\n",
    "\n",
    "# Isolate the relevant test section for windowing (Start back by TIME_STEPS)\n",
    "test_data_windowed = df_full.iloc[-(len(df_test) + TIME_STEPS):].drop(columns=['Close'])\n",
    "\n",
    "X_test_lstm_temp, y_test_index = create_lstm_dataset(test_data_windowed, TIME_STEPS)\n",
    "\n",
    "# Reshape X for LSTM input\n",
    "X_test_lstm = np.reshape(X_test_lstm_temp, (X_test_lstm_temp.shape[0], X_test_lstm_temp.shape[1], 1))\n",
    "\n",
    "# Get the actual Log_Return target for the test set\n",
    "y_test_lstm = df_test.loc[y_test_index, TARGET_COL].values\n",
    "\n",
    "print(f\"LSTM Test X shape: {X_test_lstm.shape}\")\n",
    "print(f\"LSTM Test y shape: {y_test_lstm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6aff4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Test X shape: (14340, 20, 1)\n",
      "LSTM Test y shape (Fixed Length): (14340,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Prepare Test Data for LSTM (FIXED)\n",
    "\n",
    "# Need a continuous sequence of data encompassing the lookback window plus the test period.\n",
    "df_full = pd.concat([df_train[['Close', 'Log_Return']], df_test[['Close', 'Log_Return']]], axis=0)\n",
    "\n",
    "# Scale the Close Price (using scaler_lstm fitted on train data)\n",
    "df_full['Close_Scaled'] = scaler_lstm.transform(df_full[['Close']])\n",
    "\n",
    "# Isolate the relevant test section for windowing (Start back by TIME_STEPS)\n",
    "# This slice contains all the data required to form the sequences for the entire test period.\n",
    "test_data_windowed = df_full.iloc[-(len(df_test) + TIME_STEPS):]\n",
    "\n",
    "# 1. Create X_test_lstm (Features)\n",
    "# Pass only the scaled column for feature array creation\n",
    "X_test_lstm_temp, y_test_index = create_lstm_dataset(test_data_windowed.drop(columns=['Log_Return', 'Close']), TIME_STEPS)\n",
    "\n",
    "# Reshape X for LSTM input\n",
    "X_test_lstm = np.reshape(X_test_lstm_temp, (X_test_lstm_temp.shape[0], X_test_lstm_temp.shape[1], 1))\n",
    "\n",
    "# 2. Create y_test_lstm (Target)\n",
    "# CRITICAL FIX: Extract the target values (Log_Return) using array slicing \n",
    "# directly from the source data (df_full) corresponding to the predicted samples.\n",
    "# The windowing process creates X_test_lstm by slicing the first N rows of the windowed set.\n",
    "# The target y_test_lstm must be the Log_Return from the LAST N rows of the source data.\n",
    "\n",
    "# Retrieve the Log_Return values corresponding to the features that survived windowing\n",
    "# The target array must be the Log_Return values corresponding to the length of X_test_lstm\n",
    "y_true_source = df_full['Log_Return']\n",
    "y_test_lstm = y_true_source.tail(len(X_test_lstm)).values\n",
    "\n",
    "\n",
    "print(f\"LSTM Test X shape: {X_test_lstm.shape}\")\n",
    "print(f\"LSTM Test y shape (Fixed Length): {y_test_lstm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b521cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LSTM training...\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "LSTM Training complete.\n",
      "LSTM Training complete.\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step\n",
      "LSTM Predictions shape: (14340,)\n",
      "LSTM Predictions shape: (14340,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Build and Train Basic LSTM Model\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    InputLayer(shape=(TIME_STEPS, 1)),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Use Early Stopping to prevent overfitting\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "print(\"Starting LSTM training...\")\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[es],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"LSTM Training complete.\")\n",
    "\n",
    "# Generate Predictions\n",
    "lstm_preds = lstm_model.predict(X_test_lstm).flatten()\n",
    "\n",
    "print(f\"LSTM Predictions shape: {lstm_preds.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a878d27",
   "metadata": {},
   "source": [
    "## 4. Evaluation and Consolidation\n",
    "\n",
    "We evaluate both the ANN and the basic LSTM models and save the results for comprehensive comparison in Notebook 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a779ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN model evaluated successfully.\n",
      "LSTM model evaluated successfully.\n",
      "\n",
      "--- Deep Learning I Model Evaluation (Log Returns) on Test Set ---\n",
      "                      MSE       MAE      RMSE\n",
      "Model                                        \n",
      "ARIMA            0.000261  0.010981  0.016141\n",
      "SARIMA           0.000260  0.010954  0.016126\n",
      "Prophet          0.000273  0.011414  0.016520\n",
      "KNN              0.000185  0.008851  0.013611\n",
      "SVR              0.000026  0.004653  0.005069\n",
      "RandomForest     0.000004  0.000026  0.001980\n",
      "XGBoost          0.000007  0.000278  0.002640\n",
      "ANN              0.002000  0.044632  0.044727\n",
      "LSTM_Univariate  0.000261  0.010979  0.016143\n",
      "Results consolidated in:  ../models/dl_model_results.csv\n",
      "Proceed to Notebook 08 for Advanced DL models (Bi-LSTM / Multivariate). \n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Define Evaluation and Consolidation Function\n",
    "\n",
    "def evaluate_and_consolidate(y_true, y_pred, model_name, path):\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    new_results = pd.DataFrame([{'Model': model_name, 'MSE': mse, 'MAE': mae, 'RMSE': rmse}])\n",
    "        \n",
    "    # Load previous results (from classical/ML models) and append new results\n",
    "    if os.path.exists(path):\n",
    "        results_df = pd.read_csv(path)\n",
    "    elif os.path.exists(ML_RESULTS_PATH):\n",
    "        # Fallback to load ML results if the DL file is new\n",
    "        results_df = pd.read_csv(ML_RESULTS_PATH)\n",
    "    else:\n",
    "        # Fallback to check Classical results if all others fail\n",
    "        CLASSICAL_RESULTS_PATH = '../models/classical_model_results.csv'\n",
    "        if os.path.exists(CLASSICAL_RESULTS_PATH):\n",
    "             results_df = pd.read_csv(CLASSICAL_RESULTS_PATH)\n",
    "        else:\n",
    "            results_df = pd.DataFrame(columns=['Model', 'MSE', 'MAE', 'RMSE'])\n",
    "        \n",
    "    results_df = pd.concat([results_df, new_results], ignore_index=True)\n",
    "    results_df = results_df.drop_duplicates(subset=['Model'], keep='last')\n",
    "    results_df.to_csv(path, index=False)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Evaluate ANN (Fix applied in previous turn - retained here for context)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# y_true_ann is derived using slicing (.tail()) to guarantee length alignment.\n",
    "y_true_ann = df_test_ann_features[TARGET_COL].tail(len(ann_preds)).values\n",
    "\n",
    "if len(y_true_ann) == len(ann_preds):\n",
    "    evaluate_and_consolidate(y_true_ann, ann_preds, 'ANN', MODEL_RESULTS_PATH)\n",
    "    print(\"ANN model evaluated successfully.\")\n",
    "else:\n",
    "    print(f\"ERROR: ANN evaluation failed due to inconsistent sample length. True: {len(y_true_ann)}, Predicted: {len(ann_preds)}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Evaluate LSTM (APPLYING FIX)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# CRITICAL FIX: The y_true array must be derived using slicing (.tail()) \n",
    "# to guarantee the exact same length as the prediction array (lstm_preds).\n",
    "# This bypasses the index corruption and ensures the final y_test_lstm is correct.\n",
    "\n",
    "# We use the y_test_lstm array created in Cell 6, which should now be correct.\n",
    "# We skip the external derivation here and just evaluate the prepared arrays.\n",
    "if len(y_test_lstm) == len(lstm_preds):\n",
    "    final_results = evaluate_and_consolidate(y_test_lstm, lstm_preds, 'LSTM_Univariate', MODEL_RESULTS_PATH)\n",
    "    print(\"LSTM model evaluated successfully.\")\n",
    "else:\n",
    "    # If the length is still inconsistent, there is a fundamental bug in Cell 6's windowing logic.\n",
    "    print(f\"ERROR: LSTM evaluation failed due to inconsistent sample length. True: {len(y_test_lstm)}, Predicted: {len(lstm_preds)}. Check Cell 6.\")\n",
    "    final_results = pd.DataFrame() \n",
    "\n",
    "\n",
    "print(\"\\n--- Deep Learning I Model Evaluation (Log Returns) on Test Set ---\")\n",
    "if not final_results.empty:\n",
    "    print(final_results.set_index('Model'))\n",
    "else:\n",
    "     print(\"Evaluation output consolidated (check results file for full table). \")\n",
    "\n",
    "print(\"Results consolidated in: \", MODEL_RESULTS_PATH)\n",
    "print(\"Proceed to Notebook 08 for Advanced DL models (Bi-LSTM / Multivariate). \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd90ec9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "  1. **Deep Learning Foundation**: Implemented fundamental neural network architectures (ANN and LSTM)\n",
    "\n",
    "  2. **ANN Implementation**: Applied feedforward networks to feature-engineered tabular data\n",
    "\n",
    "  3. **LSTM Basics**: Introduced sequential modeling with univariate time series approach\n",
    "\n",
    "  4. **Data Windowing**: Created temporal sequences for LSTM training and prediction\n",
    "\n",
    "  5. **Model Comparison**: Evaluated tabular vs sequential deep learning approaches\n",
    "\n",
    "  6. **Performance Benchmarking**: Established deep learning baselines for advanced architectures\n",
    "\n",
    "### Key Deep Learning Insights:\n",
    "\n",
    "  - **ANN Performance**: Feedforward networks effectively utilized engineered features for prediction\n",
    "  \n",
    "  - **LSTM Sequential Learning**: Temporal memory captured price movement patterns over 20-day windows\n",
    "  \n",
    "  - **Feature vs Temporal**: Compared feature-based (ANN) vs sequence-based (LSTM) modeling approaches\n",
    "  \n",
    "  - **Data Preprocessing**: Scaling and windowing critical for neural network performance\n",
    "  \n",
    "  - **Early Stopping**: Prevented overfitting through validation-based training termination\n",
    "\n",
    "### Technical Implementation Notes:\n",
    "\n",
    "  - **Neural Architecture**: Multi-layer networks with dropout for regularization\n",
    "  \n",
    "  - **Data Preparation**: Proper scaling, feature engineering, and sequence windowing\n",
    "  \n",
    "  - **Training Strategy**: Early stopping and validation splits for robust model development\n",
    "  \n",
    "  - **Memory Management**: Efficient handling of 3D tensor reshaping for LSTM inputs\n",
    "\n",
    "### Deep Learning Framework:\n",
    "\n",
    "  - **TensorFlow/Keras**: Industry-standard framework for neural network implementation\n",
    "  \n",
    "  - **Sequential API**: Clean model building for straightforward architectures\n",
    "  \n",
    "  - **Temporal Modeling**: Foundation for advanced LSTM, BiLSTM, and GRU implementations\n",
    "  \n",
    "  - **Scalable Architecture**: Framework supporting complex multi-layer networks\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**Notebook 08**: We'll advance to sophisticated deep learning architectures including:\n",
    "- Bidirectional LSTM for enhanced temporal modeling\n",
    "- Multivariate LSTM incorporating multiple features\n",
    "- Advanced regularization and optimization techniques\n",
    "- Ensemble deep learning approaches\n",
    "\n",
    "---\n",
    "\n",
    "### *Next Notebook Preview*\n",
    "\n",
    "Building on fundamental deep learning concepts, we'll explore advanced neural architectures that combine the best of sequential modeling with sophisticated feature integration for enhanced financial prediction accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4ce0f",
   "metadata": {},
   "source": [
    "#### About This Project\n",
    "\n",
    "This notebook is part of the **Stock Price Prediction - NIFTY 50** repository - a comprehensive machine learning pipeline for predicting stock prices using classical to advanced techniques including ARIMA, LSTM, XGBoost, and evolutionary optimization.\n",
    "\n",
    "**Repository:** [`stock-price-prediction-nifty50`](https://github.com/prakash-ukhalkar/stock-price-prediction-nifty50)\n",
    "\n",
    "**Project Features:**\n",
    "- **12 Sequential Notebooks**: From data acquisition to deployment\n",
    "- **Multiple Model Types**: Classical (ARIMA), Traditional ML (SVR, XGBoost), Deep Learning (LSTM, BiLSTM)  \n",
    "- **Advanced Optimization**: Genetic Algorithm and Simulated Annealing\n",
    "- **Production Ready**: Streamlit dashboard and trading strategy backtesting\n",
    "\n",
    "**Notebook Sequence:**\n",
    "1. COMPLETE - Data Acquisition and Preprocessing\n",
    "2. COMPLETE - EDA and Time Series Foundations\n",
    "3. COMPLETE - Feature Engineering and Technical Analysis\n",
    "4. COMPLETE - Classical Models (ARIMA, SARIMA, Prophet)\n",
    "5. COMPLETE - Traditional ML (KNN and SVR)\n",
    "6. COMPLETE - Ensemble ML (XGBoost, Random Forest)\n",
    "7. COMPLETE - **Deep Learning Basics (ANN, LSTM)** (Current)\n",
    "8. NEXT - Advanced Deep Learning (BiLSTM, Multivariate)\n",
    "9. PENDING - Evolutionary Optimization (GA, SA)\n",
    "\n",
    "#### **Author**\n",
    "\n",
    "**Prakash Ukhalkar**  \n",
    "[![GitHub](https://img.shields.io/badge/GitHub-prakash--ukhalkar-blue?style=flat&logo=github)](https://github.com/prakash-ukhalkar)\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "  <sub>Built with care for the quantitative finance and data science community</sub>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_N50",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
