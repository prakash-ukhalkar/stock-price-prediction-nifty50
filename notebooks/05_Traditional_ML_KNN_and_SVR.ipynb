{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2507a358",
   "metadata": {},
   "source": [
    "## **Stock Price Prediction - NIFTY 50**\n",
    "\n",
    "### **Notebook 05: Traditional Machine Learning (KNN and SVR)**\n",
    "\n",
    "[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/) [![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-Latest-orange)](https://scikit-learn.org/) [![Pandas](https://img.shields.io/badge/Pandas-Latest-green)](https://pandas.pydata.org/) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "---\n",
    "\n",
    "**Part of the comprehensive learning series:** [Stock Price Prediction - NIFTY 50](https://github.com/prakash-ukhalkar/stock-price-prediction-nifty50)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Implement traditional ML algorithms (KNN and SVR) for financial prediction\n",
    "- Apply Time Series Cross-Validation for robust hyperparameter tuning\n",
    "- Utilize feature engineering from technical indicators and lag variables\n",
    "- Compare supervised learning approach with classical time series methods\n",
    "- Establish ML baselines for subsequent deep learning models\n",
    "\n",
    "**Dataset Scope:** Apply supervised learning to feature-engineered training data. Predict log returns using KNN and SVR.\n",
    "\n",
    "---\n",
    "\n",
    "* This notebook implements two foundational Machine Learning algorithms —**k-Nearest Neighbors (k-NN)** and **Support Vector Regression (SVR)**—to predict the next day's **Log Return**. \n",
    "\n",
    "* This shifts our approach from statistical time series modeling to **supervised learning**, utilizing the rich set of technical indicators and lagged variables created in Notebook 03. \n",
    "\n",
    "* We use **Time Series Cross-Validation (TSCV)** for robust model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fddf34",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "We load the clean, feature-engineered training data (`nifty50_train_features.csv`) and the original raw test data (`nifty50_test.csv`). Note that the raw test data will require feature re-engineering (Step 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b9a9877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-engineered Training Data loaded. Shape: (57311, 33)\n",
      "Raw Test Data loaded. Shape: (14340, 12)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ta # Needed here for test data feature re-engineering\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Define Paths\n",
    "TRAIN_FEATURES_PATH = '../data/processed/nifty50_train_features.csv'\n",
    "TEST_DATA_PATH = '../data/processed/nifty50_test.csv'\n",
    "MODEL_RESULTS_PATH = '../models/ml_model_results.csv'\n",
    "CLASSICAL_RESULTS_PATH = '../models/classical_model_results.csv' \n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv(TRAIN_FEATURES_PATH, index_col='Date', parse_dates=True)\n",
    "# Load RAW test data (will be engineered later)\n",
    "df_test = pd.read_csv(TEST_DATA_PATH, index_col='Date', parse_dates=True)\n",
    "\n",
    "print(f\"Feature-engineered Training Data loaded. Shape: {df_train.shape}\")\n",
    "print(f\"Raw Test Data loaded. Shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd823d",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Target Definition (Training Set)\n",
    "\n",
    "The target variable ($y$) is the **Log Return**. We apply a crucial check to drop non-numeric columns from $X_{train}$ to prevent the `ValueError` during scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51465c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train initial shape: (57311, 32)\n",
      "Dropping non-numeric columns from X_train: ['Symbol']\n",
      "\n",
      "X_train final shape: (57311, 31)\n",
      "y_train shape: (57311,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Define X and y for Training (WITH TYPE CHECK AND CLEANUP)\n",
    "\n",
    "TARGET_COL = 'Log_Return'\n",
    "y_train = df_train[TARGET_COL]\n",
    "\n",
    "# X includes all other features (OHLCV, Lags, TAs)\n",
    "X_train = df_train.drop(columns=[TARGET_COL])\n",
    "\n",
    "print(f\"X_train initial shape: {X_train.shape}\")\n",
    "\n",
    "# --- CRITICAL FIX: Drop Non-Numeric Columns (Fixes ValueError: could not convert string to float) ---\n",
    "non_numeric_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "if not non_numeric_cols.empty:\n",
    "    print(f\"Dropping non-numeric columns from X_train: {non_numeric_cols.tolist()}\")\n",
    "    X_train = X_train.drop(columns=non_numeric_cols)\n",
    "else:\n",
    "    print(\"No non-numeric columns detected in X_train.\")\n",
    "\n",
    "print(f\"\\nX_train final shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b910a3a",
   "metadata": {},
   "source": [
    "## 3. Time Series Cross-Validation Setup\n",
    "\n",
    "We utilize **TimeSeriesSplit** to prevent **data leakage**, ensuring all model tuning uses training data that strictly precedes the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51505331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized TimeSeriesSplit with 5 folds for tuning.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initialize TimeSeriesSplit\n",
    "N_SPLITS = 5 \n",
    "tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "\n",
    "print(f\"Initialized TimeSeriesSplit with {N_SPLITS} folds for tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b6ea2",
   "metadata": {},
   "source": [
    "## 4. Model I: k-Nearest Neighbors Regressor (k-NN)\n",
    "\n",
    "We use a `Pipeline` to ensure the `MinMaxScaler` is applied before k-NN, as this model is sensitive to feature scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d00820eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KNN GridSearchCV...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "KNN Best Parameters: {'knn__n_neighbors': 10, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "KNN Best Score (Neg MSE): -0.00022666404417937333\n",
      "\n",
      "KNN Best Parameters: {'knn__n_neighbors': 10, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "KNN Best Score (Neg MSE): -0.00022666404417937333\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: KNN Hyperparameter Tuning with GridSearchCV and TSCV\n",
    "\n",
    "knn_pipeline = Pipeline([('scaler', MinMaxScaler()), ('knn', KNeighborsRegressor())])\n",
    "\n",
    "knn_param_grid = {\n",
    "    'knn__n_neighbors': [5, 10, 20], \n",
    "    'knn__weights': ['uniform', 'distance'], \n",
    "    'knn__p': [1, 2] \n",
    "}\n",
    "\n",
    "knn_grid = GridSearchCV(\n",
    "    estimator=knn_pipeline,\n",
    "    param_grid=knn_param_grid,\n",
    "    cv=tscv, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting KNN GridSearchCV...\")\n",
    "knn_grid.fit(X_train, y_train)\n",
    "\n",
    "knn_best_model = knn_grid.best_estimator_\n",
    "print(\"\\nKNN Best Parameters:\", knn_grid.best_params_)\n",
    "print(\"KNN Best Score (Neg MSE):\", knn_grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c785ebcc",
   "metadata": {},
   "source": [
    "## 5. Model II: Support Vector Regression (SVR)\n",
    "\n",
    "SVR, particularly with the RBF kernel, is effective for non-linear financial data. We tune the regularization (C), error margin (epsilon), and kernel coefficient (gamma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f8278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SVR GridSearchCV...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "SVR Best Parameters: {'svr__C': 1, 'svr__epsilon': 0.01, 'svr__gamma': 0.01, 'svr__kernel': 'rbf'}\n",
      "SVR Best Score (Neg MSE): -1.1234023823023779e-05\n",
      "\n",
      "SVR Best Parameters: {'svr__C': 1, 'svr__epsilon': 0.01, 'svr__gamma': 0.01, 'svr__kernel': 'rbf'}\n",
      "SVR Best Score (Neg MSE): -1.1234023823023779e-05\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: SVR Hyperparameter Tuning with GridSearchCV and TSCV\n",
    "\n",
    "svr_pipeline = Pipeline([('scaler', MinMaxScaler()), ('svr', SVR())])\n",
    "\n",
    "svr_param_grid = {\n",
    "    'svr__kernel': ['rbf'], \n",
    "    'svr__C': [0.1, 1, 10], \n",
    "    'svr__epsilon': [0.01, 0.1], \n",
    "    'svr__gamma': ['scale', 0.01]\n",
    "}\n",
    "\n",
    "svr_grid = GridSearchCV(\n",
    "    estimator=svr_pipeline,\n",
    "    param_grid=svr_param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting SVR GridSearchCV...\")\n",
    "svr_grid.fit(X_train, y_train)\n",
    "\n",
    "svr_best_model = svr_grid.best_estimator_\n",
    "print(\"\\nSVR Best Parameters:\", svr_grid.best_params_)\n",
    "print(\"SVR Best Score (Neg MSE):\", svr_grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc686eaf",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation on Final Test Set\n",
    "\n",
    "We evaluate the optimized models on the unseen test data. **CRITICAL FIX:** Because the raw test data (`df_test`) lacks the features engineered in Notebook 03, we must re-engineer and align the test features here to match the columns used during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dda61050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data re-engineered and cleaned. Final X_test shape: (14254, 31)\n",
      "Predictions generated for KNN and SVR on the test set.\n",
      "Predictions generated for KNN and SVR on the test set.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Prepare Test Data, Re-Engineer Features, and Generate Predictions\n",
    "\n",
    "# 1. Feature Re-Engineering (MUST match Notebook 03 logic)\n",
    "df_test_features = df_test.copy()\n",
    "\n",
    "LAG_PERIODS = [1, 2, 3, 5, 10]\n",
    "for lag in LAG_PERIODS:\n",
    "    df_test_features[f'Close_Lag_{lag}'] = df_test_features['Close'].shift(lag)\n",
    "    df_test_features[f'Return_Lag_{lag}'] = df_test_features['Log_Return'].shift(lag)\n",
    "\n",
    "WINDOW_TREND = [10, 20, 50] \n",
    "for window in WINDOW_TREND:\n",
    "    df_test_features[f'SMA_{window}'] = ta.trend.sma_indicator(df_test_features['Close'], window=window, fillna=False)\n",
    "    df_test_features[f'EMA_{window}'] = ta.trend.ema_indicator(df_test_features['Close'], window=window, fillna=False)\n",
    "macd = ta.trend.MACD(df_test_features['Close'], window_fast=12, window_slow=26, window_sign=9, fillna=False)\n",
    "df_test_features['MACD_Line'] = macd.macd()\n",
    "df_test_features['MACD_Signal'] = macd.macd_signal()\n",
    "RSI_WINDOW = 14 \n",
    "df_test_features[f'RSI_{RSI_WINDOW}'] = ta.momentum.rsi(df_test_features['Close'], window=RSI_WINDOW, fillna=False)\n",
    "df_test_features['MFI'] = ta.volume.money_flow_index(df_test_features['High'], df_test_features['Low'], df_test_features['Close'], df_test_features['Volume'], window=14, fillna=False)\n",
    "df_test_features['ATR'] = ta.volatility.average_true_range(df_test_features['High'], df_test_features['Low'], df_test_features['Close'], window=14, fillna=False)\n",
    "\n",
    "df_test_features = df_test_features.dropna()\n",
    "\n",
    "# 2. Final Data Cleanup and Alignment\n",
    "\n",
    "# Extract X_test and y_test from the newly featured data\n",
    "y_test = df_test_features[TARGET_COL]\n",
    "X_test = df_test_features.drop(columns=[TARGET_COL])\n",
    "\n",
    "# --- CRITICAL FIX 3: Ensure X_test is cleaned of non-numeric columns and aligned (Fixes: Unseen/Missing Features) ---\n",
    "non_numeric_cols_test = X_test.select_dtypes(include=['object', 'category']).columns\n",
    "if not non_numeric_cols_test.empty:\n",
    "    X_test = X_test.drop(columns=non_numeric_cols_test)\n",
    "\n",
    "# Align the order and presence of features to match the exact set used to train the models (X_train.columns)\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "print(f\"Test data re-engineered and cleaned. Final X_test shape: {X_test.shape}\")\n",
    "\n",
    "# 3. Prediction\n",
    "knn_preds = knn_best_model.predict(X_test)\n",
    "svr_preds = svr_best_model.predict(X_test)\n",
    "\n",
    "print(\"Predictions generated for KNN and SVR on the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d29cb870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Traditional ML Model Evaluation (Log Returns) on Test Set ---\n",
      "              MSE       MAE      RMSE\n",
      "Model                                \n",
      "ARIMA    0.000261  0.010981  0.016141\n",
      "SARIMA   0.000260  0.010954  0.016126\n",
      "Prophet  0.000273  0.011414  0.016520\n",
      "KNN      0.000185  0.008851  0.013611\n",
      "SVR      0.000026  0.004653  0.005069\n",
      "Results consolidated in:  ../models/ml_model_results.csv\n",
      "Proceed to Notebook 06 for Ensemble ML models (XGBoost/Random Forest).\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Final Evaluation and Consolidation\n",
    "\n",
    "def evaluate_and_consolidate(y_true, y_pred, model_name, path):\n",
    "    \n",
    "    # Synchronization is inherent here as X_test/y_test were cleaned in step 6.\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    new_results = pd.DataFrame([{'Model': model_name, 'MSE': mse, 'MAE': mae, 'RMSE': rmse}])\n",
    "        \n",
    "    # Load previous results (from classical models) and append new results\n",
    "    if os.path.exists(path):\n",
    "        results_df = pd.read_csv(path)\n",
    "    elif os.path.exists(CLASSICAL_RESULTS_PATH):\n",
    "        # Fallback to load classical results if the ML file doesn't exist yet\n",
    "        results_df = pd.read_csv(CLASSICAL_RESULTS_PATH)\n",
    "    else:\n",
    "        results_df = pd.DataFrame(columns=['Model', 'MSE', 'MAE', 'RMSE'])\n",
    "        \n",
    "    results_df = pd.concat([results_df, new_results], ignore_index=True)\n",
    "    results_df = results_df.drop_duplicates(subset=['Model'], keep='last')\n",
    "    results_df.to_csv(path, index=False)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Evaluate and Save Results\n",
    "evaluate_and_consolidate(y_test, knn_preds, 'KNN', MODEL_RESULTS_PATH)\n",
    "final_results = evaluate_and_consolidate(y_test, svr_preds, 'SVR', MODEL_RESULTS_PATH)\n",
    "\n",
    "print(\"--- Traditional ML Model Evaluation (Log Returns) on Test Set ---\")\n",
    "print(final_results.set_index('Model'))\n",
    "\n",
    "print(\"Results consolidated in: \", MODEL_RESULTS_PATH)\n",
    "print(\"Proceed to Notebook 06 for Ensemble ML models (XGBoost/Random Forest).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9965722",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "  1. **Supervised Learning Transition**: Shifted from statistical to ML-based approach using engineered features\n",
    "\n",
    "  2. **KNN Implementation**: Applied distance-based learning with optimal hyperparameter tuning\n",
    "\n",
    "  3. **SVR Modeling**: Implemented non-linear regression with RBF kernel optimization\n",
    "\n",
    "  4. **Time Series Cross-Validation**: Used TSCV to prevent data leakage during hyperparameter search\n",
    "\n",
    "  5. **Feature Engineering Pipeline**: Re-engineered test data features to match training set\n",
    "\n",
    "  6. **Performance Evaluation**: Calculated robust metrics on unseen test data\n",
    "\n",
    "### Key Machine Learning Insights:\n",
    "\n",
    "  - **Feature Engineering Value**: Technical indicators and lag variables enhanced predictive power\n",
    "  \n",
    "  - **KNN Performance**: Distance-based learning captured local patterns in feature space\n",
    "  \n",
    "  - **SVR Effectiveness**: Non-linear kernel regression handled complex financial relationships\n",
    "  \n",
    "  - **Cross-Validation**: TSCV ensured temporal integrity during model selection\n",
    "  \n",
    "  - **Scaling Importance**: Normalization critical for distance-based algorithms\n",
    "\n",
    "### Technical Implementation Notes:\n",
    "\n",
    "  - **Pipeline Architecture**: Integrated scaling and modeling for clean workflow\n",
    "  \n",
    "  - **Hyperparameter Tuning**: GridSearchCV with temporal splits for robust optimization\n",
    "  \n",
    "  - **Data Alignment**: Proper synchronization between predictions and test targets\n",
    "  \n",
    "  - **Feature Consistency**: Ensured test data features match training features exactly\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**Notebook 06**: We'll advance to ensemble Machine Learning models including:\n",
    "- XGBoost for gradient boosting performance\n",
    "- Random Forest for ensemble learning\n",
    "- Advanced hyperparameter optimization\n",
    "- Feature importance analysis and interpretation\n",
    "\n",
    "---\n",
    "\n",
    "### *Next Notebook Preview*\n",
    "\n",
    "Building on traditional ML foundations, we'll explore ensemble methods that combine multiple learners for enhanced prediction accuracy, representing the next evolution in our modeling progression toward deep learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336da50e",
   "metadata": {},
   "source": [
    "#### About This Project\n",
    "\n",
    "This notebook is part of the **Stock Price Prediction - NIFTY 50** repository - a comprehensive machine learning pipeline for predicting stock prices using classical to advanced techniques including ARIMA, LSTM, XGBoost, and evolutionary optimization.\n",
    "\n",
    "**Repository:** [`stock-price-prediction-nifty50`](https://github.com/prakash-ukhalkar/stock-price-prediction-nifty50)\n",
    "\n",
    "**Project Features:**\n",
    "- **12 Sequential Notebooks**: From data acquisition to deployment\n",
    "- **Multiple Model Types**: Classical (ARIMA), Traditional ML (SVR, XGBoost), Deep Learning (LSTM, BiLSTM)  \n",
    "- **Advanced Optimization**: Genetic Algorithm and Simulated Annealing\n",
    "- **Production Ready**: Streamlit dashboard and trading strategy backtesting\n",
    "\n",
    "\n",
    "#### **Author**\n",
    "\n",
    "**Prakash Ukhalkar**  \n",
    "[![GitHub](https://img.shields.io/badge/GitHub-prakash--ukhalkar-blue?style=flat&logo=github)](https://github.com/prakash-ukhalkar)\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "  <sub>Built with care for the quantitative finance and data science community</sub>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_N50",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
